{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a2072496-08d7-4647-a90d-3a3774866f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gazpacho\n",
      "  Downloading gazpacho-1.1.tar.gz (7.9 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: gazpacho\n",
      "  Building wheel for gazpacho (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gazpacho: filename=gazpacho-1.1-py3-none-any.whl size=7483 sha256=628c49bc8aa56da7e05902dcb2cac9d2dc29a3449367acddf529d98ad56b564c\n",
      "  Stored in directory: /Users/shaungutierrez/Library/Caches/pip/wheels/ec/45/e0/490eb5e25601b4f9425fcde4a0034601c492a29e82268be4d3\n",
      "Successfully built gazpacho\n",
      "Installing collected packages: gazpacho\n",
      "Successfully installed gazpacho-1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U gazpacho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ce74dedf-3609-4aae-8e04-8ea61a297f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from warnings import warn\n",
    "from IPython.core.display import clear_output\n",
    "from gazpacho import Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "031acb1c-eb78-4593-8cf6-f30c8c9145c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.ResultSet'>\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "#import get to call a get request on the site\n",
    "from requests import get\n",
    "\n",
    "#get the first page of the east bay housing prices\n",
    "response = get('https://losangeles.craigslist.org/search/eby/apa?hasPic=1&availabilityMode=0') #get rid of those lame-o's that post a housing option without a pic using their filter\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#get the macro-container for the housing posts\n",
    "posts = html_soup.find_all('li', class_= 'result-row')\n",
    "print(type(posts)) #to double check that I got a ResultSet\n",
    "print(len(posts)) #to double check I got 120 (elements/page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a1c1519-266c-4a80-9856-ae69fc9c12b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab the first post\n",
    "post_one = posts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "93d2a735-78e0-418c-a6ef-c02d65f8031e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$2,579'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grab the price of the first post\n",
    "post_one_price = post_one.a.text\n",
    "post_one_price.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64370e01-0e7d-4778-821c-005797e98d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab the time of the post in datetime format to save on cleaning efforts\n",
    "post_one_time = post_one.find('time', class_= 'result-date')\n",
    "post_one_datetime = post_one_time['datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e584a89-4863-481a-a9cc-17855339baf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#title is a and that class, link is grabbing the href attribute of that variable\n",
    "post_one_title = post_one.find('a', class_='result-title hdrlnk')\n",
    "post_one_link = post_one_title['href']\n",
    "\n",
    "#easy to grab the post title by taking the text element of the title variable\n",
    "post_one_title_text = post_one_title.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45d58391-04d4-44a8-bad2-5bf4d3e5db98",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c70c6c7c543e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpost_one_num_bedrooms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpost_one\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'housing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpost_one_sqft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpost_one\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'housing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#cleans the ft2 at the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#grabs the whole segment of housing details. We will need missing value handling in the loop as this kind of detail is not common in posts\n",
    "#the text can be split, and we can use indexing to grab the elements we want. number of bedrooms is the first element.\n",
    "#sqft is the third element\n",
    "\n",
    "post_one_num_bedrooms = post_one.find('span', class_ = 'housing').text.split()[0]\n",
    "\n",
    "post_one_sqft = post_one.find('span', class_ = 'housing').text.split()[2][:-3] #cleans the ft2 at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7066421-e4bb-48d0-ac1f-94cb6a8bfce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the neighborhood is grabbed by finding the span class 'result-hood' and pulling the text element from that\n",
    "post_one_hood = posts[0].find('span', class_='result-hood').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdbfce1-75e6-4838-ad4a-00bb065052f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build out the loop\n",
    "from time import sleep\n",
    "import re\n",
    "from random import randint #avoid throttling by not sending too many requests one after the other\n",
    "from warnings import warn\n",
    "from time import time\n",
    "from IPython.core.display import clear_output\n",
    "import numpy as np\n",
    "\n",
    "#find the total number of posts to find the limit of the pagination\n",
    "results_num = html_soup.find('div', class_= 'search-legend')\n",
    "results_total = int(results_num.find('span', class_='totalcount').text) #pulled the total count of posts as the upper bound of the pages array\n",
    "\n",
    "#each page has 119 posts so each new page is defined as follows: s=120, s=240, s=360, and so on. So we need to step in size 120 in the np.arange function\n",
    "pages = np.arange(0, results_total+1, 120)\n",
    "\n",
    "iterations = 0\n",
    "\n",
    "post_timing = []\n",
    "post_hoods = []\n",
    "post_title_texts = []\n",
    "bedroom_counts = []\n",
    "sqfts = []\n",
    "post_links = []\n",
    "post_prices = []\n",
    "lats = []\n",
    "longs = []\n",
    "\n",
    "for page in pages:\n",
    "    \n",
    "    #get request\n",
    "    response = get(\"https://losangeles.craigslist.org/d/apartments-housing-for-rent/search/apa\" \n",
    "                   + \"s=\" #the parameter for defining the page number \n",
    "                   + str(page) #the page number in the pages array from earlier\n",
    "                   + \"&hasPic=1\"\n",
    "                   + \"&availabilityMode=0\")\n",
    "\n",
    "    sleep(randint(1,5))\n",
    "     \n",
    "    #throw warning for status codes that are not 200\n",
    "    if response.status_code != 200:\n",
    "        warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "        \n",
    "    #define the html text\n",
    "    page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    #define the posts\n",
    "    posts = html_soup.find_all('li', class_= 'result-row')\n",
    "        \n",
    "    #extract data item-wise\n",
    "    for post in posts:\n",
    "\n",
    "        if post.find('span', class_ = 'result-hood') is not None:\n",
    "\n",
    "            #posting date\n",
    "            #grab the datetime element 0 for date and 1 for time\n",
    "            post_datetime = post.find('time', class_= 'result-date')['datetime']\n",
    "            post_timing.append(post_datetime)\n",
    "\n",
    "            #neighborhoods\n",
    "            post_hood = post.find('span', class_= 'result-hood').text\n",
    "            post_hoods.append(post_hood)\n",
    "\n",
    "            #title text\n",
    "            post_title = post.find('a', class_='result-title hdrlnk')\n",
    "            post_title_text = post_title.text\n",
    "            post_title_texts.append(post_title_text)\n",
    "\n",
    "            #post link\n",
    "            post_link = post_title['href']\n",
    "            post_links.append(post_link)\n",
    "                              \n",
    "            sleep(randint(1,5))\n",
    "\n",
    "            response = get(post_link)\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            coords = soup.find_all(attrs={\"data-latitude\": True, \"data-longitude\": True})\n",
    "\n",
    "            try:\n",
    "                lat = coord['data-latitude']\n",
    "            except:\n",
    "                lat = None\n",
    "            lats.append(lat)\n",
    "\n",
    "            try: \n",
    "                long = coord['data-longitude']\n",
    "            except:\n",
    "                long = None\n",
    "            longs.append(long)\n",
    "\n",
    "            #removes the \\n whitespace from each side, removes the currency symbol, and turns it into an int\n",
    "            post_price = int(post.a.text.strip().replace(\"$\", \"\").replace(\",\", \"\")) \n",
    "            post_prices.append(post_price)\n",
    "            \n",
    "            if post.find('span', class_ = 'housing') is not None:\n",
    "                \n",
    "                #if the first element is accidentally square footage\n",
    "                if 'ft2' in post.find('span', class_ = 'housing').text.split()[0]:\n",
    "                    \n",
    "                    #make bedroom nan\n",
    "                    bedroom_count = np.nan\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                    \n",
    "                    #make sqft the first element\n",
    "                    sqft = int(post.find('span', class_ = 'housing').text.split()[0][:-3])\n",
    "                    sqfts.append(sqft)\n",
    "                    \n",
    "                #if the length of the housing details element is more than 2\n",
    "                elif len(post.find('span', class_ = 'housing').text.split()) > 2:\n",
    "                    \n",
    "                    #therefore element 0 will be bedroom count\n",
    "                    bedroom_count = post.find('span', class_ = 'housing').text.replace(\"br\", \"\").split()[0]\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                    \n",
    "                    #and sqft will be number 3, so set these here and append\n",
    "                    sqft = int(post.find('span', class_ = 'housing').text.split()[2][:-3])\n",
    "                    sqfts.append(sqft)\n",
    "                    \n",
    "                #if there is num bedrooms but no sqft\n",
    "                elif len(post.find('span', class_ = 'housing').text.split()) == 2:\n",
    "                    \n",
    "                    #therefore element 0 will be bedroom count\n",
    "                    bedroom_count = post.find('span', class_ = 'housing').text.replace(\"br\", \"\").split()[0]\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                    \n",
    "                    #and sqft will be number 3, so set these here and append\n",
    "                    sqft = np.nan\n",
    "                    sqfts.append(sqft)                    \n",
    "                \n",
    "                else:\n",
    "                    bedroom_count = np.nan\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                \n",
    "                    sqft = np.nan\n",
    "                    sqfts.append(sqft)\n",
    "                \n",
    "            #if none of those conditions catch, make bedroom nan, this won't be needed    \n",
    "            else:\n",
    "                bedroom_count = np.nan\n",
    "                bedroom_counts.append(bedroom_count)\n",
    "                \n",
    "                sqft = np.nan\n",
    "                sqfts.append(sqft)\n",
    "\n",
    "                \n",
    "    iterations += 1\n",
    "    print(\"Page \" + str(iterations) + \" scraped successfully!\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Scrape complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff7daf5-07c7-4e9e-8728-30bfde4ee407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "la_apts = pd.DataFrame({'posted': post_timing,\n",
    "                       'neighborhood': post_hoods,\n",
    "                       'post title': post_title_texts,\n",
    "                       'number bedrooms': bedroom_counts,\n",
    "                        'sqft': sqfts,\n",
    "                        'URL': post_links,\n",
    "                       'price': post_prices,\n",
    "                       'Latitude': lats,\n",
    "                       'Longitude': longs})\n",
    "print(la_apts.info())\n",
    "la_apts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "316a0406-5d22-4318-83bb-e7a37b0c748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first things first, drop duplicate URLs because people are spammy on Craigslist. \n",
    "#Let's see how many uniqe posts we really have.\n",
    "la_apts = la_apts.drop_duplicates(subset='URL')\n",
    "len(la_apts.drop_duplicates(subset='URL'))\n",
    "\n",
    "#make the number bedrooms to a float (since np.nan is a float too)\n",
    "la_apts['number bedrooms'] = la_apts['number bedrooms'].apply(lambda x: float(x))\n",
    "\n",
    "#convert datetime string into datetime object to be able to work with it\n",
    "from datetime import datetime\n",
    "\n",
    "la_apts['posted'] = pd.to_datetime(la_apts['posted'])\n",
    "\n",
    "#Looking at what neighborhoods there are with la_apts['neighborhood'].unique() allowed me to see what\n",
    "#I needed to deal with in terms of cleaning those.\n",
    "\n",
    "#remove the parenthesis from the left and right of the neighborhoods\n",
    "la_apts['neighborhood'] = la_apts['neighborhood'].map(lambda x: x.lstrip('(').rstrip(')'))\n",
    "\n",
    "#titlecase them\n",
    "la_apts['neighborhood'] = la_apts['neighborhood'].str.title()\n",
    "\n",
    "#just take the first name of the neighborhood list, splitting on the '/' delimiter\n",
    "la_apts['neighborhood'] = la_apts['neighborhood'].apply(lambda x: x.split('/')[0])\n",
    "\n",
    "#remove whitespaces\n",
    "la_apts['neighborhood'] = la_apts['neighborhood'].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "8100aed1-29f0-4c65-965c-cb9258136630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(la_apts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "26da0bb3-fe02-4023-ae7c-1b9847846b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "la_apts.to_csv('Resources/la_apts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0a7dbf97-fd01-4a7f-b1dc-1ded0df1ac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "listings = la_apts['URL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "dfd89e20-53fd-460f-8b1d-215c61f84aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lats = []\n",
    "longs = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "df43c377-1806-4b82-9a91-502adb4866cb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-cfe8a413f9ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m la_apts = pd.DataFrame({'posted': post_timing,\n\u001b[0m\u001b[1;32m      2\u001b[0m                        \u001b[0;34m'neighborhood'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpost_hoods\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                        \u001b[0;34m'post title'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpost_title_texts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                        \u001b[0;34m'number bedrooms'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbedroom_counts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                         \u001b[0;34m'sqft'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msqfts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         ]\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"arrays must all be same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "la_apts = pd.DataFrame({'posted': post_timing,\n",
    "                       'neighborhood': post_hoods,\n",
    "                       'post title': post_title_texts,\n",
    "                       'number bedrooms': bedroom_counts,\n",
    "                        'sqft': sqfts,\n",
    "                        'URL': post_links,\n",
    "                       'price': post_prices,\n",
    "                       'Latitude': lats,\n",
    "                       'Longitude': longs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ccb881a0-830a-480c-b81a-1e27acf8c3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3120"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(post_timing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "3c2e37f3-48ae-455f-bc6f-41fdd0564246",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3120"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(post_hoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1e206d82-273a-4a74-b215-bac6444810ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3120"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bedroom_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "89725bed-55f7-42c3-b0ae-6c8eaf6ad76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ab5ac1e1-148f-43a2-8bbc-792e9b89591f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 120 entries, 0 to 119\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   posted           120 non-null    datetime64[ns]\n",
      " 1   neighborhood     120 non-null    object        \n",
      " 2   post title       120 non-null    object        \n",
      " 3   number bedrooms  103 non-null    float64       \n",
      " 4   sqft             107 non-null    float64       \n",
      " 5   URL              120 non-null    object        \n",
      " 6   price            120 non-null    int64         \n",
      "dtypes: datetime64[ns](1), float64(2), int64(1), object(3)\n",
      "memory usage: 7.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(la_apts.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab66708-6caa-483b-98f5-7d1f9731b636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
